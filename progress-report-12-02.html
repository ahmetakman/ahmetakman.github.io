<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Markdown file for the progress report of 12/02/2025 | Ahmet Akman</title> <meta name="author" content="Ahmet Akman"> <meta name="description" content="M2 Master's Student at Université Paris-Saclay. Master Thesis Intern at ETHZurich."> <meta name="keywords" content="theoretical and computational neuroscience, learning systems, event-based vision, neuromorphic vision, computer vision"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%B7&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ahmetakman.github.io/progress-report-12-02"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ahmet </span>Akman</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Markdown file for the progress report of 12/02/2025</h1> <p class="post-description"></p> </header> <article> <hr> <p>author: Ahmet Akman date: 12/02/2025 —</p> <h2 id="progress-report">Progress Report</h2> <ul> <li>Implemetation of the apical multiplier to the network.</li> <li>Implementation of the gradient descent on apical multipliers.</li> <li>Implementation of the learning rule and weight offloading.</li> <li>Implementation of the multiiterative learning stages.</li> <li>Characterization of the apical learning rule.</li> </ul> <h3 id="first-pass-phase-1">First pass (Phase 1):</h3> <p>Setting all $a=1$ and passing through the dataset portion $D_A$ all the gradients relating the apical multipliers are stored/recorded. Then taking the mean of through the all batches, for layer $k$ apical multipliers are set according to \(\Delta\underbar{a}_k = - \frac{1}{\#batch} \sum_{batch} \frac{\partial \mathcal{L}(\underbar{\underbar{W}}_{init}, x_{batch})}{\partial \underbar{a}_k}\)</p> <p>Then the same data $D_W$ passed through the apical multiplier mapped network and change in the loss is measured. Then the following plot for different learning rate and batch sizes obtained.</p> <p><img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/lr_batch_loss_diff.png" alt="alt text" title="Loss Drop"></p> <h4 id="apical-multiplier-gradient-statistics">Apical multiplier gradient statistics</h4> <p>To get an intuition about how the gradients associeted with the apical multipliers are distributed during the first pass (no update involved) over the whole data following histogram is obtained.</p> <ul> <li>Layer close to output -&gt; higher variation. <img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/gradient_histogram.png" alt=""> </li> </ul> <h3 id="learning-rule--weight-offloading-phase-2">Learning rule / Weight offloading (Phase 2):</h3> <p>The apical learning rule is imposed onto the network via the other part of the dataset $D_W$. Further drop in loss observed. Within a single data pass, following typical batch loss within in a single epoch are recorded and plotted.</p> <p><img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/loss_typical_phase_2.png" alt="alt text" title="Single Weight Offloading"></p> <h3 id="multi-epoch-training">Multi-epoch training</h3> <p>To be able to get an idea which levels of loss drop we can get different set of learning rates for fixed batch size (30) is applied and loss values over 100 epochs are plotted. It is typical that around the losses (on $D_W$ or on $D_{test}$) of 0.2 it is likely for values to blow up.To be able to see if this can be prevented by loss scheduling, cosine scheduling is applied to same set of starting learning rates. That did not effect much.</p> <ul> <li> <em>each line represents a learning rate pair.</em> <img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/sweep_lr_a_w_no_scheduling_test_10_02_2025.png" alt=""> <img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/sweep_lr_a_w_cosine_scheduling_test_10_02_2025.png" alt=""> To see if having smaller batch size effect: following plot suggest this allows to go lower loss values yet it has lower <img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/sweep_lr_a_w_no_scheduling_test_bs_1_11_02_2025.png" alt=""> </li> </ul> <p>Fundamentally the early stopping (higher lr pairs) runs have losses exploding earlier. **side note 1: a training is stopped if an indefinite value (NaN or inf) is observed in loss. **side note 2: a training</p> <h3 id="multi-epoch-training-multiple-phase-2-operation-before-reiteration-on-phase-1">Multi-epoch training ***multiple phase 2 operation before reiteration on phase 1</h3> <p>To see how more iterations of phase 2 before the new epoch run on phase 1 results in loss movements , a set of runs are done.</p> <h4 id="one-phase-1-operation-per-six-phase-2-operation">One phase 1 operation per six phase 2 operation</h4> <p>This case resulted in a continueing loss drop profile. <img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/multi_phase_2_12_02_2025.png" alt=""></p> <h4 id="single-phase-1-and-many-phase-2-operation">Single phase 1 and many phase 2 operation</h4> <p>Similarly for a single phase 1 and many phase to (basically until explosion) following figure shows this case. <img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/multi_phase_2_100_epoch_12_02_2025.png" alt=""></p> <h3 id="delta-w_mn-dynamics-and-selection-of-apical-multiplier-ranges">$\Delta W_{mn}$ Dynamics and selection of apical multiplier ranges</h3> <p>To get a glimpse of how and when $\Delta W_{mn}$ evolve to zero. A <em>toy</em> experiment is run with randomly initialized weights and inputs to a layer. Then via different random initalization of apical multipliers experiments are run. For example for a set of random apical multipliers $a \in [0, 1)$ it is observed that $\Delta W_{mn}$ always go to zero (that is not a certain statement just an observation after repetitive trials). Also for a set of random apical multipliers that are selected from standard gaussian distribution and it has been seen that in this case the convergence is not always there.</p> <ul> <li>Sample case where apical multiplier are <em>good</em> for weigth change <img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/learning_rule_a_01.png" alt=""> </li> <li>Sample case where apical multipliers are <em>bad</em> for weight change. <img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/learning_rule_a_randn.png" alt="a gaussian"> </li> </ul> <h3 id="comparison-of-variance-of-apical-multipliers-per-layer-during-multi-epoch-training">Comparison of variance of apical multipliers per layer during multi-epoch training</h3> <p>To get an idea about if all apical multipliers move alltogether and generate diversity along a layer, their varience per layer is measured in two settings per layer.</p> <ul> <li>Regular training (one phase 1 and one phase 2 per epoch) <img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/variance_regular_train.png" alt="varience of apical regular"> </li> <li> <p>Other training setup (one phase 1 per six phase 2 per epoch) <img src="https://ahmetakman.github.io/assets/progress_reports/12_02_25/figures/variance_multi_phase_2_train.png" alt="variance of apical multi phase 2"></p> </li> <li>Even though it might not be exactly fair comparison when they are just one go runs, it can be said when we look at the variance in epoch 10, regular setup has larger variance for last layer and similar variances for other layers. <h2 id="appendix">Appendix</h2> <h3 id="neuron-layer-model">Neuron Layer Model</h3> </li> </ul> <p>\(r_n = a_n \phi\left(\sum Wx\right)\) \(\mathcal{L}(W, a, x) = \sum_s l (\hat{y}, y(W, a, x))\) Where the $\phi$ is the activation function, $l$ is the loss function, $\hat{y}$ is the predicted output, $y$ is the output of the network, $a$ is the apical input and $inp$ is the input to the network.</p> <h3 id="learning-rule">Learning Rule</h3> <p>Using the notation from the overleaf document, for the weights between layer m and n, the learning rule is given by: \(\Delta W_{mn} \propto r_m (\tilde{r}_n - r_n)\) where $r_m$ is the rate of the presynaptic neuron and $\tilde{r_n}$ is the baseline rate of the postsynaptic neuron. Basically \(a_n r_n = \tilde{r}_n\)</p> <p>Using the notation from the paper under review: \(\Delta w = \eta r_{pre} r_{post}^{baseline}(a^* - a^{baseline})\)</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Ahmet Akman. open for PhD research opportunities </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>